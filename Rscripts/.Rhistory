prime
prime <- c()
for (i in c(1:100)) {
l = length(c(2:sqrt(i)))
for (j in c(2:l))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
if (s == l ){
prime = c(prime,i)
}
}
prime
x
prime <- c()
for (i in c(1:100)) {
l = length(c(2:sqrt(i)))
for (j in c(2:l))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
s
}
if (s == l ){
prime = c(prime,i)
}
}
i
l = length(c(2:sqrt(i)))
l
i %% j != 0
i
j
100  for (j in c(2:l))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
for (j in c(2:l))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
l
j
i
for (j in c(2:l))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
s
for (j in c(2:sqrt(i)))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
s
i
j
100 %% 3
for (j in c(2:sqrt(i)))  {
s = 0
if (i %% j != 0) {
s = s + 1
}
}
i = 100
s = 0
for (j in c(2:sqrt(i)))  {
if (i %% j != 0) {
s = s + 1
}
}
s
i = 13
s = 0
for (j in c(2:sqrt(i)))  {
if (i %% j != 0) {
s = s + 1
}
}
s
c(2:sqrt(i))
l = length(c(2:sqrt(i)))
l
prime <- c()
for (i in c(1:100)) {
l = length(c(2:sqrt(i)))
s = 0
for (j in c(2:sqrt(i)))  {
if (i %% j != 0) {
s = s + 1
}
}
if (s == l){
prime = c(prime,i)
}
}
prime
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file
filePath = "C:/Users/isaac/Downloads/text.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Plot word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Read the text file
filePath = "C:/Users/isaac/Downloads/text.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("que", "por", "los", "para"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
# Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Plot word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
View(docs)
# Read the text file
filePath = "C:/Users/isaac/Downloads/text.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
text <- readLines(filePath)
text
# Read the text file
text <- readLines(file.choose())
# Read the text file
filePath = "C:/Users/isaac/Downloads/text.txt"
text <- read.delim(filePath)
View(text)
text <- readLines(filePath, encoding = "UTF-8")
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("que", "por", "los", "para"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Plot word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
View(d)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=1500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Plot word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=1500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=600, height=350)
dev.off()
ot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=600, height=350)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=1500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
# Plot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=600, height=350)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=1500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=1500, height=900)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=1500, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=1500, height=900)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=1500, height=900)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file
filePath = "C:/Users/isaac/Downloads/texto.txt"
text <- readLines(filePath, encoding = "UTF-8")
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("que", "por", "los", "para"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("que", "por", "los", "para"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
# Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# Plot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=1500, height=900)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
# Plot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=900, height=600)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=400, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
# Plot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=900, height=600)
wordcloud(words = d$word, freq = d$freq, min.freq = 50,
max.words=400, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
# Plot word cloud
set.seed(1234)
png(file="C:/Users/isaac/Downloads/sopa_letras.png",
width=900, height=600)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=400, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
Sys.which("make")
install.packages("jsonlite", type = "source")
library(gdata)
library(tidyverse)
a ?3
a=3
#load ggmap
library(httr)
library(ggmap)
library(ggplot2)
library(tidyverse)
library(devtools)
library(osrm)
# PACKAGES
library(tidyverse) # data wrangling
library(stringr)   # work with strings
library(sf)        # geospatial
library(geosphere) # calculate distances
library(haven)     # read/write .dta (Stata)
library(here)      # use relative filepaths
library(osrm)      # calculate road distances
library(assertthat)
# old spatial packages since osrm works with those
library(sp) #spatial objects; used by rgdal
setwd('C:/Users/isaac/Dropbox/Apps/ShareLaTeX/metodos_estadisticos/Rscripts')
geocoded <- read.csv('../DB/geocoded_p2.csv', stringsAsFactors = FALSE) %>%
drop_na()
View(geocoded)
geocoded_sp <- st_as_sf(geocoded ,coords = c("lon","lat"),crs=4326, sf_column_name="geometry") %>%
as('Spatial')
# Manually get it using address on Google Maps:
jlca <- st_sf(
folio = "0000-0000" ,
demandado = 0,
addresses = "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geoAddress= "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geometry = st_sfc(st_point(c(-99.14541650000001, 19.4245109)))
) %>%
st_set_crs(4326) %>% # because the point was taken from Google Maps coordinates
mutate(folio = as.character(folio),
addresses = as.character(addresses),
geoAddress = as.character(geoAddress)
) %>%
as('Spatial')
# Use OSRM's server
# options(osrm.server = "http://router.project-osrm.org/")
# Use local instance
options(osrm.server = "http://localhost:5000/")
for(i in 1:nrow(geocoded)) {
tryCatch({
result <- osrmRoute(src = geocoded[i,c("folio","lon","lat")], dst = jlca, overview= FALSE)
geocoded$duration[i] <- as.numeric(result[1])
geocoded$distance[i] <- as.numeric(result[2])
}, error=function(e){})
}
View(geocoded)
# Use OSRM's server
options(osrm.server = "http://router.project-osrm.org/")
for(i in 1:nrow(geocoded)) {
tryCatch({
result <- osrmRoute(src = geocoded[i,c("folio","lon","lat")], dst = jlca, overview= FALSE)
geocoded$duration[i] <- as.numeric(result[1])
geocoded$distance[i] <- as.numeric(result[2])
}, error=function(e){})
}
#load ggmap
library(httr)
library(ggmap)
library(ggplot2)
library(tidyverse)
library(devtools)
library(osrm)
# PACKAGES
library(tidyverse) # data wrangling
library(stringr)   # work with strings
library(sf)        # geospatial
library(geosphere) # calculate distances
library(haven)     # read/write .dta (Stata)
library(here)      # use relative filepaths
library(osrm)      # calculate road distances
library(assertthat)
# old spatial packages since osrm works with those
library(sp) #spatial objects; used by rgdal
setwd('C:/Users/isaac/Dropbox/Apps/ShareLaTeX/metodos_estadisticos/Rscripts')
geocoded <- read.csv('../DB/geocoded_p2.csv', stringsAsFactors = FALSE) %>%
drop_na()
geocoded_sp <- st_as_sf(geocoded ,coords = c("lon","lat"),crs=4326, sf_column_name="geometry") %>%
as('Spatial')
# Manually get it using address on Google Maps:
jlca <- st_sf(
folio = "0000-0000" ,
demandado = 0,
addresses = "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geoAddress= "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geometry = st_sfc(st_point(c(-99.14541650000001, 19.4245109)))
) %>%
st_set_crs(4326) %>% # because the point was taken from Google Maps coordinates
mutate(folio = as.character(folio),
addresses = as.character(addresses),
geoAddress = as.character(geoAddress)
) %>%
as('Spatial')
# Use local instance
options(osrm.server = "http://localhost:5000/")
#load ggmap
library(httr)
library(ggmap)
library(ggplot2)
library(tidyverse)
library(devtools)
library(osrm)
# PACKAGES
library(tidyverse) # data wrangling
library(stringr)   # work with strings
library(sf)        # geospatial
library(geosphere) # calculate distances
library(haven)     # read/write .dta (Stata)
library(here)      # use relative filepaths
library(osrm)      # calculate road distances
library(assertthat)
# old spatial packages since osrm works with those
library(sp) #spatial objects; used by rgdal
geocoded <- read.csv('../DB/geocoded_p2.csv', stringsAsFactors = FALSE) %>%
drop_na()
View(geocoded)
geocoded_sp <- st_as_sf(geocoded ,coords = c("lon","lat"),crs=4326, sf_column_name="geometry") %>%
as('Spatial')
# Manually get it using address on Google Maps:
jlca <- st_sf(
folio = "0000-0000" ,
demandado = 0,
addresses = "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geoAddress= "Av. Dr. R?o de la Loza 68, Doctores, 06720 Ciudad de M?xico, CDMX, Mexico",
geometry = st_sfc(st_point(c(-99.14541650000001, 19.4245109)))
) %>%
st_set_crs(4326) %>% # because the point was taken from Google Maps coordinates
mutate(folio = as.character(folio),
addresses = as.character(addresses),
geoAddress = as.character(geoAddress)
) %>%
as('Spatial')
# Use OSRM's server
#options(osrm.server = "http://router.project-osrm.org/")
# Use local instance
options(osrm.server = "http://localhost:5000/")
for(i in 1:nrow(geocoded)) {
tryCatch({
result <- osrmRoute(src = geocoded[i,c("folio","lon","lat")], dst = jlca, overview= FALSE)
geocoded$duration[i] <- as.numeric(result[1])
geocoded$distance[i] <- as.numeric(result[2])
}, error=function(e){})
}
